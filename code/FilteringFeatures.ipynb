{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e779fd3-c22f-4286-82f6-50f699f85db2",
   "metadata": {},
   "source": [
    "Filtering the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c879ee9c-733e-4a13-8219-cca7e4cc44ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joanbarreto/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy.ma as ma\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe77e53-cc3e-452e-9da8-34b25b78c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the regions -----------\n",
    "\n",
    "CircadianRegions = '../output/CircadianRegions_2kb.csv'\n",
    "CircadianRegions = pd.read_csv(CircadianRegions)\n",
    "\n",
    "\n",
    "\n",
    "#********************************************************\n",
    "#\n",
    "#           TESTING ONLY!!! (comment!)\n",
    "#\n",
    "CircadianRegions = CircadianRegions.sample(frac=0.1, replace=False, random_state=5461) # Sample a fraction of the df\n",
    "#\n",
    "#********************************************************\n",
    "\n",
    "\n",
    "\n",
    "# Prepare data -----------\n",
    "data_df = CircadianRegions[['gene','JTK_adjphase','region2kb']] # keep relevant cols\n",
    "data_df = data_df.sort_values('gene') # make sure df is sorted by gene\n",
    "# CircadianRegions = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get features -----------\n",
    "\n",
    "# Use an 8bp window and 3bp overlap\n",
    "window_size = 8\n",
    "overlap = 3\n",
    "non_overlap = window_size-overlap\n",
    "regions = data_df['region2kb']\n",
    "# Slide window and get features\n",
    "Features = regions.apply(lambda x: [x[i:i+window_size] for i in range(0, len(x)-7, non_overlap)])\n",
    "Features.name = \"features\" # Rename\n",
    "# Get unique features\n",
    "unique_features = [item for sublist in Features for item in sublist]\n",
    "unique_features = set(unique_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d46b8b15-4456-4ae1-9993-1787d6eaa069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed one-hot-encoding\n",
      "CPU times: user 3.37 s, sys: 4.58 s, total: 7.96 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# One hot encoding -----------\n",
    "\n",
    "# Create features df\n",
    "features_df = pd.concat([data_df['gene'], Features], axis=1)\n",
    "# data_df = None\n",
    "# Set and fit MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "one_hot = mlb.fit_transform(features_df[\"features\"]) # fit\n",
    "# Create a new df with the one-hot encoding and the gene column\n",
    "one_hot_df = pd.DataFrame(one_hot, columns=mlb.classes_)\n",
    "one_hot_df[\"gene\"] = features_df[\"gene\"].values\n",
    "one_hot_df = one_hot_df[[\"gene\"] + list(mlb.classes_)] # \"gene\" col to front\n",
    "one_hot_df = one_hot_df.set_index('gene') # Set gene as index\n",
    "# Remove features containin \"N\" (only 6)\n",
    "one_hot_df = one_hot_df.loc[:, ~one_hot_df.columns.str.contains('N')]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "feature_names = np.array(one_hot_df.columns) # get feature names\n",
    "gene_names = np.array(one_hot_df.index) # get gene names\n",
    "np_data = one_hot_df.values  # Convert data to numpy array\n",
    "\n",
    "print('computed one-hot-encoding')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9836b9a1-5a5b-4987-8125-d3b898bd16c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished splitting data\n",
      "CPU times: user 674 ms, sys: 683 ms, total: 1.36 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Split data for training -----------\n",
    "# Appears to be faster with a df than arrays\n",
    "\n",
    "# Data for models\n",
    "X_features = one_hot_df\n",
    "Y_target = data_df[\"JTK_adjphase\"]\n",
    "\n",
    "# .to_numpy()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, Y_target, test_size=0.5, random_state=5461)\n",
    "\n",
    "print('Finished splitting data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c0708d3e-d2aa-451f-ba26-1e666a5374ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished model  rfr\n",
      "CPU times: user 23.2 s, sys: 1.25 s, total: 24.4 s\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Select and train models -----------\n",
    "\n",
    "# Create empty dfs to output\n",
    "ModelsPerformance = pd.DataFrame()\n",
    "FeatureImportance = pd.DataFrame()\n",
    "\n",
    "# Create models\n",
    "# knn = KNeighborsRegressor(n_neighbors=5)\n",
    "# svr = SVR(kernel='rbf', C=1, gamma=0.1)\n",
    "rfr = RandomForestRegressor(n_estimators=1, random_state=0)\n",
    "# gbr = GradientBoostingRegressor(n_estimators=1, learning_rate=0.1, max_depth=1, random_state=0, loss='ls')\n",
    "# Store them in dictionary\n",
    "# Models = {'rfr':rfr,\n",
    "#           'svr':svr,\n",
    "#           'gbr':gbr,\n",
    "#           'knn':knn}\n",
    "Models = {'rfr':rfr}\n",
    "\n",
    "# Select model\n",
    "model_name = next(iter(Models)) # Name\n",
    "model = next(iter(Models.values())) # Model\n",
    "\n",
    "# Train the algorithm on the training set\n",
    "start_time = time.time() # ***** Start timer *****\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained algorithm to predict the target variable of the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Feature importance\n",
    "# if model_name == 'rfr' | model_name == 'gbr':\n",
    "feature_importance = model.feature_importances_\n",
    "# Create feature importance df for model\n",
    "colname =  model_name + '_Importance'\n",
    "FI_df = pd.DataFrame({'Feature': one_hot_df.columns, colname: feature_importance})\n",
    "FI_df = FI_df.sort_values(by=[colname], ascending=False)\n",
    "# Append to greater df\n",
    "FeatureImportance = pd.concat([FeatureImportance,FI_df], axis=0) \n",
    "\n",
    "\n",
    "end_time = time.time() # ***** End timer *****\n",
    "# Time to run\n",
    "execution_time = round(end_time - start_time, 4)\n",
    "print('Finished model ',model_name)\n",
    "\n",
    "                  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2d54be0-3a01-45e2-b4b2-e61dca1be386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance -----------\n",
    "\n",
    "y_test = np.array(y_test, dtype=float)  # convert to np array\n",
    "y_test=ma.masked_invalid(y_test)  # Treat nan\n",
    "y_pred=ma.masked_invalid(y_pred)\n",
    "\n",
    "PCC = round(np.corrcoef(y_test, y_pred)[0,1],2)\n",
    "# y_pred = np.append(y_pred, np.nan)\n",
    "# y_test = np.append(y_test, np.nan)\n",
    "\n",
    "# np.any(np.isnan(y_test))\n",
    "# np.any(np.isnan(y_pred))\n",
    "# np.all(np.isfinite(y_test))\n",
    "# np.all(np.isfinite(y_pred))\n",
    "\n",
    "y_test[np.isfinite(y_test) == True] = 0\n",
    "y_pred[np.isfinite(y_pred) == True] = 0\n",
    "y_test[np.isnan(y_test) == True] = 0\n",
    "y_pred[np.isnan(y_pred) == True] = 0\n",
    "\n",
    "                  \n",
    "# Get performance metrics into a df\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "performance_df = pd.DataFrame({'Model':model_name,\n",
    "                               'PCC':PCC,\n",
    "                               'MSE':mse,\n",
    "                               'ExecTime':execution_time},index=[0])\n",
    "# Append to performance df\n",
    "ModelsPerformance = pd.concat([ModelsPerformance,performance_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1670a-90df-417a-9fda-462f6fdb3a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336fb16-b141-49ef-af34-ead6d3c3459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on density -----------\n",
    "\n",
    "# Copy data\n",
    "df = one_hot_df\n",
    "# Remove features containin \"N\" (only 6)\n",
    "df = df.loc[:, ~df.columns.str.contains('N')]\n",
    "\n",
    "feature_names = np.array(df.columns) # get feature names\n",
    "gene_names = np.array(df.index) # get gene names\n",
    "np_data.shape = df.values  # Convert data to numpy array\n",
    "\n",
    "\n",
    "# Get densities\n",
    "feature_densities = df.sum()\n",
    "# feature_densities = df.sum() / df.shape[0]\n",
    "feature_densities = feature_densities.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Filter out bottom and top thresholds (quantile-based)\n",
    "# Calculate the nth percentile values\n",
    "density_percentiles = np.percentile(feature_densities, [0,25,50,75,100])\n",
    "print('Percentiles 0,25,50,75,100 are: \\n', str(density_percentiles))\n",
    "bottom_thresh = density_percentiles[1]\n",
    "top_thresh = density_percentiles[2]\n",
    "#filter\n",
    "filtered_series = feature_densities[\n",
    "    (feature_densities > bottom_thresh) & (feature_densities < top_thresh)]\n",
    "filtered_features = filtered_series.index.to_list() # list names\n",
    "filtered_features = df[filtered_features] # subset\n",
    "print('Dimensions after density-based filter ',str(filtered_features.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07262e-bccd-44c2-9c4f-b2ca1a4e752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based PCA -----------\n",
    "\n",
    "df = filtered_features # copy data\n",
    "print('Dimensions after density-based filter ',str(df.shape))\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(df)\n",
    "# Number of components (n_comp)\n",
    "n_comp = 10\n",
    "pc_names = ['PC{:02d}'.format(i) for i in range(1, n_comp+1)]\n",
    "pca = PCA(n_components=n_comp)\n",
    "pca.fit(X_std)\n",
    "print('Fitted PCA')\n",
    "# # transform the data to the new PCA space\n",
    "# X_pca = pca.transform(X_std)\n",
    "# # create a new dataframe with the PCA components\n",
    "# df_pca = pd.DataFrame(X_pca, columns=pc_names)\n",
    "# # add the original feature names as column names\n",
    "# df_pca.columns = pc_names\n",
    "# df_pca.index = df.index\n",
    "\n",
    "# Get loadings\n",
    "loadings = pca.components_\n",
    "df_loadings = pd.DataFrame(loadings.T, columns=pc_names, index=df.columns)\n",
    "\n",
    "# sort the rows (i.e., features) of the dataframe by their absolute loading value in descending order\n",
    "df_loadings = df_loadings.apply(lambda x: x.abs().sort_values(ascending=False), axis=0)\n",
    "\n",
    "# Empty list for selected features\n",
    "selected_features=[]\n",
    "\n",
    "for pc in pc_names:\n",
    "    pc_column = pc # current PC\n",
    "    pc_sorted = df_loadings[pc_column].sort_values(ascending=False) # sort loadings\n",
    "    half_features = int(0.1 * len(pc_sorted.index)) # Select 10% of the features\n",
    "    features_PC = list(pc_sorted[:half_features].index)\n",
    "    selected_features.extend(features_PC)\n",
    "    # len(selected_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0af13e-9dab-4dad-951d-8ef5c4bd3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_features = np.unique(selected_features)\n",
    "n_unique_features = len(unique_features) # get number of features\n",
    "print('Total features: ', n_unique_features)\n",
    "# Filtered dataset\n",
    "filtered_df = one_hot_df.loc[:, unique_features]\n",
    "# Export\n",
    "filename = '../output/FilteredFeatures_PCA_n' + str(n_unique_features) + '.csv'\n",
    "filtered_df.to_csv(filename)\n",
    "\n",
    "\n",
    "# calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed time: \", round(elapsed_time,2), \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d390981-c077-4b20-81b9-44050239aac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
